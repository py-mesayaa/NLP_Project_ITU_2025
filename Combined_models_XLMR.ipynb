{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1944b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from transformers import  XLMRobertaTokenizerFast\n",
    "from datasets import Dataset\n",
    "from transformers import AdamW\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import  DataCollatorForTokenClassification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "from torch.nn.functional import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bdd77",
   "metadata": {},
   "source": [
    "## Commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc64769",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16abae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "test_data = \"data/test_merged_output.conll\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "845b64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refs(json_path):\n",
    "    cultural_refs_tag = []\n",
    "    cultural_refs_IDs = {}\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        cultural_data = json.load(f)\n",
    "        cultural_refs_tag.extend([entry['tag'] for entry in cultural_data])\n",
    "        cultural_refs_IDs.update({entry['tag']: entry['id'] for entry in cultural_data})\n",
    "\n",
    "    return cultural_refs_tag, cultural_refs_IDs\n",
    "\n",
    "ner_tags_path = \"data/ner_tags.json\"\n",
    "cultural_ref_tags_path = \"data/cultural_tags.json\"\n",
    "\n",
    "ner_tags, tag2id_ner = extract_refs(ner_tags_path)\n",
    "cultural_tags, tag2id_cultural = extract_refs(cultural_ref_tags_path)\n",
    "\n",
    "num_labels_culture = len(tag2id_cultural)\n",
    "num_labels_ner = len(tag2id_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4730768",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_path = \"./data/ner_tags.json\"\n",
    "cultural_ref_tags_path = \"./data/cultural_tags.json\"\n",
    "\n",
    "ner_tags, tag2id_ner = extract_refs(ner_tags_path)\n",
    "cultural_tags, tag2id_cultural = extract_refs(cultural_ref_tags_path)\n",
    "\n",
    "id2tag_ner = {v: k for k, v in tag2id_ner.items()}\n",
    "id2tag_cultural = {v: k for k, v in tag2id_cultural.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35817dc5",
   "metadata": {},
   "source": [
    "## 1. Inicialization of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8654e8f",
   "metadata": {},
   "source": [
    "### 1.a. NER tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc762280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = XLMRobertaForTokenClassification.from_pretrained(\"models/xlmr-ner-head/checkpoint-3018\")\n",
    "ner_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33f156",
   "metadata": {},
   "source": [
    "### 1.a. Culture tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9d3ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cultural_model = XLMRobertaForTokenClassification.from_pretrained(\"models/xlmr-cultural-head/checkpoint-700\")\n",
    "cultural_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa1cbf",
   "metadata": {},
   "source": [
    "## 2. NER and Cultural prediction connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63950e",
   "metadata": {},
   "source": [
    "### 2.a Getting the predictions from the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7600984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Barack Obama was born in Hawaii and lived in the United States. Ivett is a student in Copenhagen. Bence is also in Copenhagen, but further.\"\n",
    "sentence = \"So, it was a chill afternoon in New York City, and Emily Roberts was kicking back at this low-key café near Central Park, waiting on Dr. Sanjay Mehta. Dude finally shows up, all jazzed about some big-deal archaeology gig happening in Athens, Greece. Apparently, Professor Laura Kim—yeah, the legend from Seoul National University—is dropping some serious knowledge bombs there. Meanwhile, way over in Tokyo, Kenji Tanaka was neck-deep in robot parts when his phone lit up. It was Maria Gonzalez hitting him up from Madrid, trying to rope him into some wild collab with NASA out in Houston, Texas. Talk about long-distance hustle. On the other side of the world, Liam O'Connor was off the grid, hiking through the misty trails of Connemara National Park in Ireland, totally ghosting his phone. Poor Sofia Petrova was blowing up his messages from her freezing office in St. Petersburg, Russia, probably thinking he got eaten by sheep or something. Back in Cape Town, Dr.Amina Yusuf was setting up for a Zoom with Michael Zhang, who just touched down in Beijing after a whirlwind week at some high-profile summit in Berlin. Man was running on fumes and airport coffee. Meanwhile, in sunny Sydney, Noah Thompson was catching up with Priya Desai over flat whites. They were cracking up, talking about their wild college days back at Oxford University—you know, late-night cramming....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8383d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Named Entities with Tags:\n",
      "New: LOC\n",
      "York: LOC\n",
      "City ,: LOC\n",
      "Emily: PER\n",
      "Roberts: PER\n",
      "Central: LOC\n",
      "Park ,: LOC\n",
      "Sanjay: PER\n",
      "Meh ta .: PER\n",
      "Athen s ,: LOC\n",
      "Greece .: LOC\n",
      "Laura: PER\n",
      "Kim — ye ah ,: PER\n",
      "Seoul: ORG\n",
      "National: ORG\n",
      "University — is: ORG\n",
      "Tokyo ,: LOC\n",
      "Ke nji: PER\n",
      "Tan aka: PER\n",
      "Maria: PER\n",
      "Go nza lez: PER\n",
      "Madrid ,: LOC\n",
      "NASA: ORG\n",
      "Houston ,: LOC\n",
      "Texas .: LOC\n",
      "Liam: PER\n",
      "O ' Con nor: PER\n",
      "Con ne mara: LOC\n",
      "National: LOC\n",
      "Park: LOC\n",
      "Ireland ,: LOC\n",
      "Sofia: PER\n",
      "Petrov a: PER\n",
      "St .: LOC\n",
      "Petersburg ,: LOC\n",
      "Russia ,: LOC\n",
      "Cape: LOC\n",
      "Town ,: LOC\n",
      "Michael: PER\n",
      "Z hang ,: PER\n",
      "Beijing: LOC\n",
      "Berlin .: LOC\n",
      "Sydney ,: LOC\n",
      "No ah: PER\n",
      "Thompson: PER\n",
      "Pri ya: PER\n",
      "Desa i: PER\n",
      "Oxford: ORG\n",
      "University — you: ORG\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, is_split_into_words=False, truncation=True)\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "attention_mask = encoding[\"attention_mask\"]\n",
    "offset_mapping = encoding[\"offset_mapping\"][0]\n",
    "word_ids = encoding.word_ids()\n",
    "encoding.pop(\"offset_mapping\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = ner_model(**encoding)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "#ensure thet the labels are the same as the models label\n",
    "id2label = ner_model.config.id2label\n",
    "\n",
    "entities = []\n",
    "current_word = None\n",
    "current_entity = None\n",
    "current_label = \"O\"\n",
    "\n",
    "for idx, word_idx in enumerate(word_ids):\n",
    "    if word_idx is None:\n",
    "        continue  # Skip special tokens\n",
    "\n",
    "    label = id2label[predictions[idx].item()]\n",
    "    word = tokenizer.convert_ids_to_tokens(int(input_ids[0][idx]))\n",
    "    \n",
    "    # Skip subword tokens (keep only first token of a word)\n",
    "    if word_idx != current_word:\n",
    "        if current_entity and current_label != \"O\":\n",
    "            entities.append((\" \".join(current_entity), current_label))\n",
    "        if label.startswith(\"B-\"):\n",
    "            current_entity = [word.lstrip(\"▁\")]\n",
    "            current_label = label[2:]\n",
    "        elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "            current_entity = [word.lstrip(\"▁\")]\n",
    "        else:\n",
    "            current_entity = []\n",
    "            current_label = \"O\"\n",
    "        current_word = word_idx\n",
    "    else:\n",
    "        # Continuation of the same word (likely subword)\n",
    "        if current_entity is not None:\n",
    "            current_entity.append(word.lstrip(\"▁\"))\n",
    "\n",
    "if current_entity and current_label != \"O\":\n",
    "    entities.append((\" \".join(current_entity), current_label))\n",
    "\n",
    "# to see if the entites are rigth:\n",
    "print(\"Extracted Named Entities with Tags:\")\n",
    "for word, tag in entities:\n",
    "    print(f\"{word}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee978d",
   "metadata": {},
   "source": [
    "### 2.b. Pass the NER entites to the Cultural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53463280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: New, NER Tag: LOC, Cultural Tag: Latin\n",
      "Entity: York, NER Tag: LOC, Cultural Tag: Eastern Asian\n",
      "Entity: City ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Emily, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Roberts, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Central, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Park ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sanjay, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Meh ta ., NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Athen s ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Greece ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Laura, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Kim — ye ah ,, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Seoul, NER Tag: ORG, Cultural Tag: Eastern Asian\n",
      "Entity: National, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: University — is, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: Tokyo ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Ke nji, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Tan aka, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Maria, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Go nza lez, NER Tag: PER, Cultural Tag: Eastern Asian\n",
      "Entity: Madrid ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: NASA, NER Tag: ORG, Cultural Tag: Latin\n",
      "Entity: Houston ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Texas ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Liam, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: O ' Con nor, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Con ne mara, NER Tag: LOC, Cultural Tag: Latin\n",
      "Entity: National, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Park, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Ireland ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sofia, NER Tag: PER, Cultural Tag: African\n",
      "Entity: Petrov a, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: St ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Petersburg ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Russia ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Cape, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Town ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Michael, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Z hang ,, NER Tag: PER, Cultural Tag: Eastern Asian\n",
      "Entity: Beijing, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Berlin ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sydney ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: No ah, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Thompson, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Pri ya, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Desa i, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Oxford, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: University — you, NER Tag: ORG, Cultural Tag: Latin\n"
     ]
    }
   ],
   "source": [
    "for entity_text, ner_tag in entities:    \n",
    "    encoding = tokenizer(\n",
    "        entity_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=32 \n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = cultural_model(**encoding)\n",
    "  \n",
    "    logits = outputs.logits[0]\n",
    "    \n",
    "    entity_logits = logits[0]\n",
    "    probs = softmax(entity_logits, dim=0)\n",
    "    predicted_id = torch.argmax(probs).item()\n",
    "    predicted_label = cultural_model.config.id2label[predicted_id]\n",
    "\n",
    "    print(f\"Entity: {entity_text}, NER Tag: {ner_tag}, Cultural Tag: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ea345a",
   "metadata": {},
   "source": [
    "## 3. Visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34183aa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
