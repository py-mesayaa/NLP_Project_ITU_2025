{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1944b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from transformers import  XLMRobertaTokenizerFast\n",
    "from datasets import Dataset\n",
    "from transformers import AdamW\n",
    "from transformers import XLMRobertaTokenizerFast, XLMRobertaForTokenClassification, DataCollatorForTokenClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "from seqeval.metrics import classification_report, precision_score, recall_score, f1_score\n",
    "\n",
    "from transformers import  DataCollatorForTokenClassification\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import warnings\n",
    "from torch.nn.functional import softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9bdd77",
   "metadata": {},
   "source": [
    "### Set up for labels and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc64769",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "845b64d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_refs(json_path):\n",
    "    cultural_refs_tag = []\n",
    "    cultural_refs_IDs = {}\n",
    "\n",
    "    with open(json_path, \"r\") as f:\n",
    "        cultural_data = json.load(f)\n",
    "        cultural_refs_tag.extend([entry['tag'] for entry in cultural_data])\n",
    "        cultural_refs_IDs.update({entry['tag']: entry['id'] for entry in cultural_data})\n",
    "\n",
    "    return cultural_refs_tag, cultural_refs_IDs\n",
    "\n",
    "ner_tags_path = \"data/ner_tags.json\"\n",
    "cultural_ref_tags_path = \"data/cultural_tags.json\"\n",
    "\n",
    "ner_tags, tag2id_ner = extract_refs(ner_tags_path)\n",
    "cultural_tags, tag2id_cultural = extract_refs(cultural_ref_tags_path)\n",
    "\n",
    "num_labels_culture = len(tag2id_cultural)\n",
    "num_labels_ner = len(tag2id_ner)\n",
    "\n",
    "id2tag_ner = {v: k for k, v in tag2id_ner.items()}\n",
    "id2tag_cultural = {v: k for k, v in tag2id_cultural.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16abae30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "test_data = \"data/test_merged_output.conll\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5222b5f",
   "metadata": {},
   "source": [
    "#### Tokenizing the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdc2cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conllu(file_path):\n",
    "    \" This function create a usable dataset of the sparse .conllu file with the NER tags and cultural refferences\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    cultural_references = []\n",
    "    \n",
    "    with open(file_path, \"r\",encoding=\"utf-8\") as file:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        cultural_ref = []\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "                       \n",
    "            if not line:\n",
    "                if tokens:  \n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(ner_tags)\n",
    "                    cultural_references.append(cultural_ref)\n",
    "              \n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "                cultural_ref = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split(\"\\t\")\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                token = parts[0]  \n",
    "                ner_tag = parts[1] \n",
    "                cultural_ref_val = parts[2]\n",
    "                \n",
    "                tokens.append(token)\n",
    "                ner_tags.append(ner_tag)\n",
    "                cultural_ref.append(cultural_ref_val)\n",
    "        \n",
    "       \n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            tags.append(ner_tags)\n",
    "            cultural_references.append(cultural_ref)\n",
    "    \n",
    "    return sentences, tags, cultural_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3660761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test, tags_test, cultural_refs_test = parse_conllu(test_data)\n",
    "test_data = [{\"tokens\": s, \"ner_tags\": t, \"cultural_ref\": c} for s, t, c in zip(sentences_test, tags_test, cultural_refs_test)]\n",
    "dataset_test = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "93ab0626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels_with_cultural(example):\n",
    "    tokenized = tokenizer(\n",
    "        example['tokens'], \n",
    "        truncation=True, \n",
    "        padding=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    cultural_ref = []\n",
    "\n",
    "    for i, word in enumerate(example['tokens']):\n",
    "        if example['ner_tags'][i] != \"O\":\n",
    "            \n",
    "            labels.append(tag2id_ner.get(example['ner_tags'][i], -100))\n",
    "            \n",
    "            cultural_ref.append(tag2id_cultural.get(example['cultural_ref'][i], -100))\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "            cultural_ref.append(-100)\n",
    "\n",
    "    tokenized['labels'] = labels\n",
    "    tokenized['cultural_ref'] = cultural_ref\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57e15df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "tokenized_cultural_test = dataset_test.map(tokenize_and_align_labels_with_cultural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3a0aee32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags', 'cultural_ref', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 2952\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_cultural_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35817dc5",
   "metadata": {},
   "source": [
    "## 1. Inicialization of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8654e8f",
   "metadata": {},
   "source": [
    "### 1.a. NER tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc762280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_model = XLMRobertaForTokenClassification.from_pretrained(\"models/xlmr-ner-head/checkpoint-3018\")\n",
    "ner_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33f156",
   "metadata": {},
   "source": [
    "### 1.a. Culture tagger "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c9d3ab1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForTokenClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cultural_model = XLMRobertaForTokenClassification.from_pretrained(\"models/xlmr-cultural-head/checkpoint-700\")\n",
    "cultural_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa1cbf",
   "metadata": {},
   "source": [
    "## 2. NER and Cultural prediction connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc63950e",
   "metadata": {},
   "source": [
    "### 2.a Getting the predictions from the NER model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d7600984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = \"Barack Obama was born in Hawaii and lived in the United States. Ivett is a student in Copenhagen. Bence is also in Copenhagen, but further.\"\n",
    "sentence = \"So, it was a chill afternoon in New York City, and Emily Roberts was kicking back at this low-key café near Central Park, waiting on Dr. Sanjay Mehta. Dude finally shows up, all jazzed about some big-deal archaeology gig happening in Athens, Greece. Apparently, Professor Laura Kim—yeah, the legend from Seoul National University—is dropping some serious knowledge bombs there. Meanwhile, way over in Tokyo, Kenji Tanaka was neck-deep in robot parts when his phone lit up. It was Maria Gonzalez hitting him up from Madrid, trying to rope him into some wild collab with NASA out in Houston, Texas. Talk about long-distance hustle. On the other side of the world, Liam O'Connor was off the grid, hiking through the misty trails of Connemara National Park in Ireland, totally ghosting his phone. Poor Sofia Petrova was blowing up his messages from her freezing office in St. Petersburg, Russia, probably thinking he got eaten by sheep or something. Back in Cape Town, Dr.Amina Yusuf was setting up for a Zoom with Michael Zhang, who just touched down in Beijing after a whirlwind week at some high-profile summit in Berlin. Man was running on fumes and airport coffee. Meanwhile, in sunny Sydney, Noah Thompson was catching up with Priya Desai over flat whites. They were cracking up, talking about their wild college days back at Oxford University—you know, late-night cramming....\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695a251",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69191cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_get_offsets(sentence, tokenizer):\n",
    "    \"\"\"\n",
    "    Tokenizes the sentence and returns input IDs, attention mask, and offsets.\n",
    "    \"\"\"\n",
    "    encoding = tokenizer(sentence, return_tensors=\"pt\", return_offsets_mapping=True, is_split_into_words=False, truncation=True)\n",
    "    input_ids = encoding[\"input_ids\"]\n",
    "    attention_mask = encoding[\"attention_mask\"]\n",
    "    offset_mapping = encoding[\"offset_mapping\"][0] \n",
    "    word_ids = encoding.word_ids()\n",
    "    encoding.pop(\"offset_mapping\") \n",
    "    \n",
    "    return input_ids, attention_mask, offset_mapping, word_ids\n",
    "\n",
    "def predict_ner(encoding, ner_model):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = ner_model(**encoding)\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)[0]\n",
    "    return predictions\n",
    "\n",
    "def extract_entities(input_ids, word_ids, predictions, tokenizer, id2label):\n",
    "\n",
    "    entities = []\n",
    "    current_word = None\n",
    "    current_entity = None\n",
    "    current_label = \"O\"\n",
    "\n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue  # Skip special tokens\n",
    "\n",
    "        label = id2label[predictions[idx].item()]\n",
    "        word = tokenizer.convert_ids_to_tokens(int(input_ids[0][idx]))\n",
    "        \n",
    "        # Skip subword tokens (keep only the first token of a word)\n",
    "        if word_idx != current_word:\n",
    "            if current_entity and current_label != \"O\":\n",
    "                entities.append((\" \".join(current_entity), current_label))\n",
    "            if label.startswith(\"B-\"):\n",
    "                current_entity = [word.lstrip(\"▁\")]\n",
    "                current_label = label[2:]\n",
    "            elif label.startswith(\"I-\") and current_label == label[2:]:\n",
    "                current_entity.append(word.lstrip(\"▁\"))\n",
    "            else:\n",
    "                current_entity = []\n",
    "                current_label = \"O\"\n",
    "            current_word = word_idx\n",
    "        else:\n",
    "            # Continuation of the same word (likely subword)\n",
    "            if current_entity is not None:\n",
    "                current_entity.append(word.lstrip(\"▁\"))\n",
    "\n",
    "    # Append the last entity if any\n",
    "    if current_entity and current_label != \"O\":\n",
    "        entities.append((\" \".join(current_entity), current_label))\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b9ab7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Named Entities with Tags:\n",
      "New: LOC\n",
      "New York: LOC\n",
      "New York City ,: LOC\n",
      "Emily: PER\n",
      "Emily Roberts: PER\n",
      "Central: LOC\n",
      "Central Park ,: LOC\n",
      "Sanjay: PER\n",
      "Sanjay Meh ta .: PER\n",
      "Athen s ,: LOC\n",
      "Greece .: LOC\n",
      "Laura: PER\n",
      "Laura Kim — ye ah ,: PER\n",
      "Seoul: ORG\n",
      "Seoul National: ORG\n",
      "Seoul National University — is: ORG\n",
      "Tokyo ,: LOC\n",
      "Ke nji: PER\n",
      "Ke nji Tan aka: PER\n",
      "Maria: PER\n",
      "Maria Go nza lez: PER\n",
      "Madrid ,: LOC\n",
      "NASA: ORG\n",
      "Houston ,: LOC\n",
      "Texas .: LOC\n",
      "Liam: PER\n",
      "Liam O ' Con nor: PER\n",
      "Con ne mara: LOC\n",
      "Con ne mara National: LOC\n",
      "Con ne mara National Park: LOC\n",
      "Ireland ,: LOC\n",
      "Sofia: PER\n",
      "Sofia Petrov a: PER\n",
      "St .: LOC\n",
      "St . Petersburg ,: LOC\n",
      "Russia ,: LOC\n",
      "Cape: LOC\n",
      "Cape Town ,: LOC\n",
      "Michael: PER\n",
      "Michael Z hang ,: PER\n",
      "Beijing: LOC\n",
      "Berlin .: LOC\n",
      "Sydney ,: LOC\n",
      "No ah: PER\n",
      "No ah Thompson: PER\n",
      "Pri ya: PER\n",
      "Pri ya Desa i: PER\n",
      "Oxford: ORG\n",
      "Oxford University — you: ORG\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_mask, offset_mapping, word_ids = tokenize_and_get_offsets(sentence, tokenizer)\n",
    "\n",
    "predictions = predict_ner({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, ner_model)\n",
    "\n",
    "id2label = ner_model.config.id2label\n",
    "entities = extract_entities(input_ids, word_ids, predictions, tokenizer, id2label)\n",
    "\n",
    "print(\"Extracted Named Entities with Tags:\")\n",
    "for word, tag in entities:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ee978d",
   "metadata": {},
   "source": [
    "### 2.b. Pass the NER entites to the Cultural model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d8a82d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[-1.2601,  3.3004,  0.5962,  0.6516,  1.1875,  0.1818, -2.2296,\n",
       "          -0.1563, -1.0434, -1.3488],\n",
       "         [-0.9783,  0.5731, -2.5981, -0.5789, -2.1552,  4.4398, -0.9211,\n",
       "           2.4038,  1.1361, -1.4119],\n",
       "         [-0.8675,  1.6432, -2.4132, -0.2053, -1.9445,  4.1493, -1.3196,\n",
       "           2.1429,  0.5877, -1.5297],\n",
       "         [-2.8530,  4.5201,  2.3758,  0.5015,  0.1321,  0.3668, -2.8938,\n",
       "          -0.7940, -0.9317, -3.4310],\n",
       "         [-2.6802,  4.1746,  2.7399,  0.3276,  2.2364, -0.8926, -2.7958,\n",
       "          -1.1117, -1.3645, -3.2229],\n",
       "         [-1.0831,  2.7994,  0.2827,  0.6256,  1.0628,  0.0573, -1.8229,\n",
       "          -0.1489, -1.0642, -1.0602]]]), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "53463280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: New, NER Tag: LOC, Cultural Tag: Latin\n",
      "Entity: New York, NER Tag: LOC, Cultural Tag: Eastern Asian\n",
      "Entity: New York City ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Emily, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Emily Roberts, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Central, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Central Park ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sanjay, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Sanjay Meh ta ., NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Athen s ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Greece ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Laura, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Laura Kim — ye ah ,, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Seoul, NER Tag: ORG, Cultural Tag: Eastern Asian\n",
      "Entity: Seoul National, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: Seoul National University — is, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: Tokyo ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Ke nji, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Ke nji Tan aka, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Maria, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Maria Go nza lez, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Madrid ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: NASA, NER Tag: ORG, Cultural Tag: Latin\n",
      "Entity: Houston ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Texas ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Liam, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Liam O ' Con nor, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Con ne mara, NER Tag: LOC, Cultural Tag: Latin\n",
      "Entity: Con ne mara National, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Con ne mara National Park, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Ireland ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sofia, NER Tag: PER, Cultural Tag: African\n",
      "Entity: Sofia Petrov a, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: St ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: St . Petersburg ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Russia ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Cape, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Cape Town ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Michael, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Michael Z hang ,, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Beijing, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Berlin ., NER Tag: LOC, Cultural Tag: African\n",
      "Entity: Sydney ,, NER Tag: LOC, Cultural Tag: African\n",
      "Entity: No ah, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: No ah Thompson, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Pri ya, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Pri ya Desa i, NER Tag: PER, Cultural Tag: Latin\n",
      "Entity: Oxford, NER Tag: ORG, Cultural Tag: African\n",
      "Entity: Oxford University — you, NER Tag: ORG, Cultural Tag: African\n"
     ]
    }
   ],
   "source": [
    "for entity_text, ner_tag in entities:    \n",
    "    encoding = tokenizer(\n",
    "        entity_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=32 \n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = cultural_model(**encoding)\n",
    "  \n",
    "    logits = outputs.logits[0]\n",
    "    \n",
    "    entity_logits = logits[0]\n",
    "    probs = softmax(entity_logits, dim=0)\n",
    "    predicted_id = torch.argmax(probs).item()\n",
    "    predicted_label = cultural_model.config.id2label[predicted_id]\n",
    "\n",
    "    print(f\"Entity: {entity_text}, NER Tag: {ner_tag}, Cultural Tag: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
