{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60ebad5e",
   "metadata": {},
   "source": [
    "#### 1. Converting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de0cd368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def convert_spanish_to_conll(input_path, output_path):\\n    with open(input_path, \\'r\\', encoding=\\'utf-8\\') as infile,          open(output_path, \\'w\\', encoding=\\'utf-8\\') as outfile:\\n\\n        current_sentence = None\\n\\n        for line in infile:\\n            parts = line.strip().split()\\n            if not parts or len(parts) < 4:\\n                continue \\n\\n            sentence_id = parts[0]\\n            # word = parts[2]\\n            ner_tag = parts[-1]  \\n\\n            if current_sentence is None:\\n                current_sentence = sentence_id\\n            elif sentence_id != current_sentence:\\n                outfile.write(\"\\n\")\\n                current_sentence = sentence_id\\n\\n            outfile.write(f\"{word} {ner_tag}\\n\")\\n\\n        outfile.write(\"\\n\")  \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def convert_spanish_to_conll(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        current_sentence = None\n",
    "\n",
    "        for line in infile:\n",
    "            parts = line.strip().split()\n",
    "            if not parts or len(parts) < 4:\n",
    "                continue \n",
    "\n",
    "            sentence_id = parts[0]\n",
    "            # word = parts[2]\n",
    "            ner_tag = parts[-1]  \n",
    "\n",
    "            if current_sentence is None:\n",
    "                current_sentence = sentence_id\n",
    "            elif sentence_id != current_sentence:\n",
    "                outfile.write(\"\\n\")\n",
    "                current_sentence = sentence_id\n",
    "\n",
    "            outfile.write(f\"{word} {ner_tag}\\n\")\n",
    "\n",
    "        outfile.write(\"\\n\")  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2d3581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_spanish_to_conll(\"ensemble.conll-2002.txt\", \"spanish.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a173279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def convert_ewt_to_conll(input_path, output_path):\\n    with open(input_path, \\'r\\', encoding=\\'utf-8\\') as infile,          open(output_path, \\'w\\', encoding=\\'utf-8\\') as outfile:\\n\\n        for line in infile:\\n            line = line.strip()\\n            if not line or line.startswith(\"#\"):\\n                if line == \"\":\\n                    outfile.write(\"\\n\")  \\n                continue  \\n\\n            parts = line.split(\\'\\t\\')\\n            if len(parts) < 3:\\n                continue\\n\\n            token = parts[1]\\n            tag = parts[2]\\n\\n            outfile.write(f\"{token} {tag}\\n\")\\n\\n        outfile.write(\"\\n\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def convert_ewt_to_conll(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "\n",
    "        for line in infile:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\"):\n",
    "                if line == \"\":\n",
    "                    outfile.write(\"\\n\")  \n",
    "                continue  \n",
    "\n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "\n",
    "            token = parts[1]\n",
    "            tag = parts[2]\n",
    "\n",
    "            outfile.write(f\"{token} {tag}\\n\")\n",
    "\n",
    "        outfile.write(\"\\n\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc06a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert_ewt_to_conll(\"en_ewt-ud-train.iob2\", \"ewt.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f2da792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def clean_and_merge_conll_files(folder_path, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        files = glob.glob(os.path.join(folder_path, \"*.conll\"))\n",
    "\n",
    "        for file_path in files:\n",
    "            with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "                for line in infile:\n",
    "                    line = line.strip()\n",
    "                    if not line:\n",
    "                        outfile.write(\"\\n\")\n",
    "                        continue\n",
    "\n",
    "                    parts = line.split()\n",
    "                    if len(parts) < 2:\n",
    "                        continue \n",
    "\n",
    "                    word = parts[0]\n",
    "                    tag = parts[1]  \n",
    "\n",
    "                    outfile.write(f\"{word} {tag}\\n\")\n",
    "\n",
    "                outfile.write(\"\\n\")  \n",
    "\n",
    "    print(f\"Merged {len(files)} files into {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fae04788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 0 files into combined_training.conll\n"
     ]
    }
   ],
   "source": [
    "clean_and_merge_conll_files(\"./data/train\", \"combined_training.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c5a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll_data(conll_path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(conll_path, encoding=\"utf-8\") as f:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "                continue\n",
    "            token, tag = line.split()\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        if tokens:  \n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c900fef",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.data/combined_training.conll'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sentences, tags \u001b[38;5;241m=\u001b[39m \u001b[43mload_conll_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.data/combined_training.conll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m, in \u001b[0;36mload_conll_data\u001b[1;34m(conll_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m sentences \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconll_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      6\u001b[0m     tags \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\coding\\.venv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.data/combined_training.conll'"
     ]
    }
   ],
   "source": [
    "sentences, tags = load_conll_data(\".data/combined_training.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1198f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_conllu(file_path):\n",
    "    \" This function create a usable dataset of the sparse .conllu file with the NER tags and cultural refferences\"\n",
    "    sentences = []\n",
    "    tags = []\n",
    "    cultural_references = []\n",
    "    \n",
    "    with open(file_path, \"r\",encoding=\"utf-8\") as file:\n",
    "        tokens = []\n",
    "        ner_tags = []\n",
    "        cultural_ref = []\n",
    "        \n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "                       \n",
    "            if not line:\n",
    "                if tokens:  \n",
    "                    sentences.append(tokens)\n",
    "                    tags.append(ner_tags)\n",
    "                    cultural_references.append(cultural_ref)\n",
    "              \n",
    "                tokens = []\n",
    "                ner_tags = []\n",
    "                cultural_ref = []\n",
    "                continue\n",
    "\n",
    "            parts = line.split(\"\\t\")\n",
    "            \n",
    "            if len(parts) >= 3:\n",
    "                token = parts[0]  \n",
    "                ner_tag = parts[1] \n",
    "                cultural_ref_val = parts[2]\n",
    "                \n",
    "                tokens.append(token)\n",
    "                ner_tags.append(ner_tag)\n",
    "                cultural_ref.append(cultural_ref_val)\n",
    "        \n",
    "       \n",
    "        if tokens:\n",
    "            sentences.append(tokens)\n",
    "            tags.append(ner_tags)\n",
    "            cultural_references.append(cultural_ref)\n",
    "    \n",
    "    return sentences, tags, cultural_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54a04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences,tags,cultural_tags= parse_conllu(\"./data/train_merged_output.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32e653",
   "metadata": {},
   "source": [
    "#### 2. Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1667237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([tag for seq in tags for tag in seq])\n",
    "tag2id = {tag: i for i, tag in enumerate(label_encoder.classes_)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b6209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\coding\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\coding\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def tokenize_and_align(sentences, tags):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        sentences,\n",
    "        is_split_into_words=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    labels_aligned = []\n",
    "    for i, label in enumerate(tags):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                aligned.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                aligned.append(tag2id[label[word_idx]])\n",
    "            else:\n",
    "                aligned.append(-100)  # ignore subword pieces\n",
    "            previous_word_idx = word_idx\n",
    "        labels_aligned.append(aligned)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_aligned\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ca852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "data = [{\"tokens\": s, \"ner_tags\": t} for s, t in zip(sentences, tags)]\n",
    "dataset = Dataset.from_list(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7a5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\coding\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaTokenizerFast\n",
    "\n",
    "tokenizer = XLMRobertaTokenizerFast.from_pretrained(\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4204d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "    )\n",
    "    word_ids = tokenized.word_ids()\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(tag2id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            label_ids.append(-100)  \n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized[\"labels\"] = label_ids\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7612557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/16093 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(tokenize_and_align_labels)\n",
    "tokenized_val = val_dataset.map(tokenize_and_align_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf3e3aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['I', 'beg', 'to', 'differ', '.'],\n",
       " 'ner_tags': ['O', 'O', 'O', 'O', 'O'],\n",
       " 'input_ids': [0,\n",
       "  87,\n",
       "  32834,\n",
       "  47,\n",
       "  129927,\n",
       "  6,\n",
       "  5,\n",
       "  2,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [-100,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train[0]\n",
    "tokenized_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<s>', 'IGN'), ('▁Thanks', 'O'), ('▁', 'O'), (',', 'IGN'), ('</s>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN')]\n",
      "========================================\n",
      "[('<s>', 'IGN'), ('▁Do', 'O'), ('▁n', 'O'), (\"'\", 'IGN'), ('t', 'IGN'), ('▁let', 'O'), ('▁them', 'O'), ('▁dim', 'O'), ('▁your', 'O'), ('▁light', 'O'), ('</s>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN')]\n",
      "========================================\n",
      "[('<s>', 'IGN'), ('▁le', 'B-PER'), ('mel', 'IGN'), ('pe', 'IGN'), ('@', 'IGN'), ('NU', 'IGN'), ('.', 'IGN'), ('COM', 'IGN'), ('</s>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN'), ('<pad>', 'IGN')]\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenized_train[i]['input_ids'])\n",
    "    labels = [id2tag.get(id, \"IGN\") for id in tokenized_train[i]['labels']]\n",
    "    print(list(zip(tokens, labels)))\n",
    "    print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e903f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import XLMRobertaForTokenClassification\n",
    "\n",
    "num_labels = len(tag2id)\n",
    "\n",
    "model = XLMRobertaForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2tag,\n",
    "    label2id=tag2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eebefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from seqeval.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2tag[p] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2tag[l] for (p, l) in zip(pred, label) if l != -100]\n",
    "        for pred, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(true_labels, true_predictions),\n",
    "        \"recall\": recall_score(true_labels, true_predictions),\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33000858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./xlmr-ner-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be41f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3018' max='3018' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3018/3018 43:30, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.819508</td>\n",
       "      <td>0.828571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.042500</td>\n",
       "      <td>0.048568</td>\n",
       "      <td>0.866419</td>\n",
       "      <td>0.851413</td>\n",
       "      <td>0.858851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.025900</td>\n",
       "      <td>0.046787</td>\n",
       "      <td>0.863845</td>\n",
       "      <td>0.855971</td>\n",
       "      <td>0.859890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3018, training_loss=0.06768012601040388, metrics={'train_runtime': 2611.8882, 'train_samples_per_second': 18.483, 'train_steps_per_second': 1.155, 'total_flos': 3153790136982528.0, 'train_loss': 0.06768012601040388, 'epoch': 3.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4857785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='297' max='112' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112/112 57:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04678715020418167, 'eval_precision': 0.8638454461821528, 'eval_recall': 0.8559708295350957, 'eval_f1': 0.85989010989011, 'eval_runtime': 14.1801, 'eval_samples_per_second': 126.163, 'eval_steps_per_second': 7.898, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdf1c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 6 files into ./data/combined_testing.conll\n"
     ]
    }
   ],
   "source": [
    "########### TESTING PHASE ###########\n",
    "\n",
    "clean_and_merge_conll_files(\"./data/test/\", \"./data/combined_testing.conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e8ec47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cefd1cd3a35440b8992f1fe05417e8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2952 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_sentences, test_tags = load_conll_data(\"./data/combined_testing.conll\")\n",
    "\n",
    "test_data = [{\"tokens\": s, \"ner_tags\": t} for s, t in zip(test_sentences, test_tags)]\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "tokenized_test = test_dataset.map(tokenize_and_align_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d578a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.10240750014781952,\n",
       " 'eval_precision': 0.5111876075731497,\n",
       " 'eval_recall': 0.42428571428571427,\n",
       " 'eval_f1': 0.4637002341920375,\n",
       " 'eval_runtime': 22.0239,\n",
       " 'eval_samples_per_second': 134.036,\n",
       " 'eval_steps_per_second': 8.4,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(eval_dataset=tokenized_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
